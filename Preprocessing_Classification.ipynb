{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load packages:\n",
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression as Logistic\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import*\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and data transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf = pd.read_json(\"train.json\")\n",
    "testdf = pd.read_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dictionaries of ingredients/cuisines and list of ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cuis_ingr = {}\n",
    "ingr_list=[]\n",
    "# Create dict with ket = cuisine and value = list of ingred.\n",
    "for a,b in traindf.groupby('cuisine'):\n",
    "    #done this way remos issue of list within list of ingr\n",
    "    cuis_ingr[a] = list(itertools.chain.from_iterable(b['ingredients'].values))\n",
    "    ingr_list+=list(itertools.chain.from_iterable(b['ingredients'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create list of ingredients to remove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique ingredients\n",
    "unique_ingr = pd.Series(ingr_list).value_counts()\n",
    "# list ingredients that appear only one time in the dataset\n",
    "ingr_rm= unique_ingr[unique_ingr<2].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of Ingredients in Training dataset:',len(ingr_list))\n",
    "print('Number of Unique Ingredients in Training dataset:',len(np.unique(ingr_list)))\n",
    "print('Number of Ingredients to be removed from dataset', len(ingr_rm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean train dataset and create strings of ingredients:\n",
    "When using the preproces_ing function we can specify True if we want the one_word routine to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_word = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove ingredients that appear only once\n",
    "traindf['ingredients_rm'] = traindf.apply(lambda row: remove_ing(row['ingredients'],ingr_rm), axis=1)\n",
    "# clean and prepare list of ingredients as a single string\n",
    "traindf['ingredients_string'] = traindf.apply(lambda row: preprocess_ing(row['ingredients_rm'], one_word), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdf['ingredients_rm'] = testdf.apply(lambda row: remove_ing(row['ingredients'],ingr_rm), axis=1)\n",
    "testdf['ingredients_string'] = testdf.apply(lambda row: preprocess_ing(row['ingredients_rm'],one_word), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Results of preprocessing routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change setting for pd column width\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf.ix[:2, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf.ix[95:100,[2,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Corpus of ingredients for train adn test and Vecotrize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_train = traindf['ingredients_string']\n",
    "corpus_test = testdf['ingredients_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,1), \n",
    "                             analyzer='word', max_df=0.5, token_pattern=r'\\w+')\n",
    "\n",
    "# Create the input matrix and the label vector for the train set:\n",
    "X_train = vectorizer.fit_transform(corpus_train)\n",
    "#train_vector_feat = vectorizer.get_feature_names()\n",
    "y_train = traindf['cuisine']\n",
    "\n",
    "# Create input matrix for the test set: \n",
    "X_test = vectorizer.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check shape of input matrix obtained:\n",
    "print('Train', X_train.shape)\n",
    "print('Test', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stratified k-fold method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classifier:\n",
    "\n",
    "use one_word preprocessing routine for best performance with Logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call classifier:\n",
    "lc = Logistic(solver='liblinear', C=5, penalty='l2', class_weight = 'balanced', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# cross validation score for the classifier selected:\n",
    "logis_score= cross_val_score(lc, X_train, y_train, scoring='f1_weighted', \n",
    "                        cv=k_fold, n_jobs=-1)\n",
    "\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (logis_score.mean(), logis_score.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of small Randomized search for the optimal parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'solver':['liblinear','lbfgs']\n",
    "              'penalty': ['l1','l2'],\n",
    "              'C': np.linspace(1,100)}\n",
    "\n",
    "lc_rnd = RandomizedSearchCV(lc, param_grid, cv=5, n_iter=20)\n",
    "lc_rnd.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score:', lm_rnd.best_score_)\n",
    "print('Best Parameters:', lm_rnd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use best parameters to train algorithm, fit the test set and output the csv file for the submission to kaggle.\n",
    "If the randomized search was not performace the classifier set above can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if hyperparameter search was performed:\n",
    "#lc_model = lm_rnd.best_estimator_\n",
    "# if we already have parameters:\n",
    "lc_model = lc\n",
    "\n",
    "# fit the train set\n",
    "lc_model.fit(X_train, y_train)\n",
    "\n",
    "# predicting test cuisines\n",
    "prediction_logis = lc_model.predict(X_test)\n",
    "\n",
    "# Create CSV file:\n",
    "sub_logis = testdf[['id']].copy()\n",
    "sub_logis['cuisine']= prediction_logis\n",
    "sub_logis.to_csv(\"submission_logistic_TESTSSS.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes:\n",
    "\n",
    "use one_word preprocessing routine for best performance with NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call classifier:\n",
    "nb = MultinomialNB(alpha=0.103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_score= cross_val_score(nb, X_train, y_train, scoring='accuracy', \n",
    "                        cv=k_fold, n_jobs=-1)\n",
    "\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (nb_score.mean(), nb_score.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of small Randomized search for the optimal parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'alpha': np.linspace(0.001,1)}\n",
    "\n",
    "nb_rnd = RandomizedSearchCV(nb, param_grid, cv=5, n_iter=20)\n",
    "nb_rnd.fit(X_train, y_train)\n",
    "\n",
    "print(nb_rnd.best_score_)\n",
    "print(nb_rnd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use best parameters to train algorithm, fit the test set and output the csv file for the submission to kaggle.\n",
    "If the randomized search was not performace the classifier set above can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if hyperparameter search was performed:\n",
    "#nb_model = nb_rnd.best_estimator_\n",
    "# if we already have parameters:\n",
    "nb_model = nb\n",
    "# fit train set\n",
    "nb_model.fit(X_train, y_train)\n",
    "# predict test set\n",
    "nb_predictions= nb_model.predict(X_test)\n",
    "# create file for submission:\n",
    "sub_nb=testdf[['id']].copy()\n",
    "sub_nb['cuisine']=nb_predictions\n",
    "sub_nb.to_csv(\"submission_multiNB_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create classifier:\n",
    "svc_model = SVC(C=15,gamma=1, kernel='rbf', class_weight= 'balanced',\n",
    "                decision_function_shape='ovr', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# cross validation score for parameters selected:\n",
    "svc_score= cross_val_score(svc_model, X_train, y_train, scoring='f1_weighted', \n",
    "                        cv=k_fold, n_jobs=-1)\n",
    "\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (svc_score.mean(), svc_score.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit train data\n",
    "svc_model.fit(X_train, y_train)\n",
    "# predict labels for test set:\n",
    "prediction_svc = svc_model.predict(X_test)\n",
    "# create csv file for submission:\n",
    "sub_svc = testdf[['id']].copy()\n",
    "sub_svc['cuisine']= prediction_svc\n",
    "sub_svc.to_csv(\"submission_svc_c15_g1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create classifier:\n",
    "forest = RandomForestClassifier(criterion='gini',n_estimators = 1000,max_features= 'auto',\n",
    "                                         class_weight='balanced', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evalueate the algorithm and the parameters selected:\n",
    "score_forest = cross_val_score(forest, X_train, y_train, scoring='f1_weighted', cv=5)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (score_forest.mean(), score_forest.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit train data\n",
    "forest.fit(X_train, y_train)\n",
    "# predict cuisines for test set\n",
    "prediction_forest= forest.predict(test_vector)\n",
    "#Create file for submission\n",
    "sub_forest = testdf[['id']].copy()\n",
    "sub_forest['cuisine']= prediction_forest\n",
    "sub_forest.to_csv(\"submission_forest.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Confusion Matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating a confusion matrix it is necessary to restart the kernel, do the preprocessing and finally set the algorithm we want to use for the task in the cells above.\n",
    "After that we can split the train data set in a train and validation set, train the model, and use the validation set to create a more realistic confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split train set in train and validation\n",
    "x_tr, x_val, y_tr, y_val=tts(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "# declare the model we want to evaluate, for example:\n",
    "model = lc #could be svc_model, nb\n",
    "\n",
    "# train the model:\n",
    "model.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the confusion matrix and the normalized version:\n",
    "cm = confusion_matrix(y_val, model.predict(x_val))\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# create list of cuisines to label axis\n",
    "cuisines = traindf['cuisine'].value_counts().index.tolist()\n",
    "xt=yt=sorted(cuisines)\n",
    "\n",
    "# plot the figure and save it:\n",
    "plt.figure(figsize=(12,10))\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(cm_normalized, square=True,xticklabels=xt, yticklabels=yt,\n",
    "                     cmap='YlGnBu', annot=True, fmt='.2f',linewidths=.5)\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_xlabel('Predicted label')\n",
    "#ax.figure.savefig(\"output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
